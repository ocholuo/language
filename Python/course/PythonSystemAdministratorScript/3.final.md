
# Python 3 Scripting for System Administrators

[toc]

From the linuxacademy: [Python 3 Scripting for System Administrators](https://linuxacademy.com/cp/courses/lesson/course/1680/lesson/1)


---

## The Project

We have many database servers that we manage
create a single tool that can use to easily back up the databases to either AWS S3 or locally.

We would like to be able to:
- Specify the database URL to backup.
- Specify a “driver” (local or s3)
- Specify the backup “destination”. This will be a file path for local and a bucket name for s3.
- Depending on the “driver”, create a local backup of the database or upload the backup to an S3 bucket.


Links For This Video:
- [db_setup.sh](https://raw.githubusercontent.com/linuxacademy/content-python3-sysadmin/master/helpers/db_setup.sh)
- PostgreSQL RPM

---

## install

### Sett up PostgreSQL Lab Server

need a PostgreSQL database to work with.

```c
$ curl -o db_setup.sh https://raw.githubusercontent.com/linuxacademy/content-python3-sysadmin/master/helpers/db_setup.sh
$ chmod +x db_setup.sh
$ ./db_setup.sh
```

### Instal The Postgres 9.6 Client in dev pc

```c
$ wget https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
$ sudo yum install -y pgdg-redhat-repo-latest.noarch.rpm
$ sudo yum update -y
$ sudo yum autoremove -y postgresql
$ sudo yum install -y postgresql96
```

```c
# Install the repository RPM:
dnf install https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm

# Disable the built-in PostgreSQL module:
dnf -qy module disable postgresql

# Install PostgreSQL:
dnf install postgresql96-server

# Optionally initialize the database and enable automatic start:
/usr/pgsql-9.6/bin/postgresql96-setup initdb
systemctl enable postgresql-9.6
systemctl start postgresql-9.6
```

### Test connection from Workstation

```c
$ psql postgres://[USERNAME]:[PASSWORD]@[SERVER_IP]:80/sample -c "SELECT count(id) FROM employees;"

$ psql postgres://user:passwd@54.183.238.187:80/sample -c "SELECT count(id) FROM employees;"
 count
-------
  1000
(1 row)
```

---

## Planning Through Documentation

### Creating the Repo and Virtualenv

building a project that will be more than a single file, create a full project complete with source control and dependencies.

### 1. start by creating the directory for project

```c
$ rm ~/requirements.txt
$ mkdir -p ~/code/pgbackup
$ cd ~/code/pgbackup
```

### 2. setup virtualenv
- `pip` and `virtualenvs`: allow to manage dependency versions.
- `pipenv`: a new tool to manage project’s virtualenv and install dependencies.

```c
// install pipenv for user
// create a Python 3 virtualenv for project

$ pip3.6 install --user pipenv

$ pipenv --python $(which python3.6)
✔ Successfully created virtual environment!
Virtualenv location: /home/server/.local/share/virtualenvs/pgbackup-uP_oEm6K
Creating a Pipfile for this project…
// Rather than creating a requirements.txt file
// pipenv has created a Pipfile: to store virtualenv and dependency information.
// To activate our new virtualenv, use the command pipenv shell
// to deactivate, use exit instead of deactivate.
```

### 3. set up git
- set up git as our source control management tool by initializing our repository.
- add a `.gitignore` file from GitHub so that we don’t later track files that we don’t mean to.

```c
$ git init
$ curl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore -o .gitignore
```

### 4. Sketch out the `README.rst`
- start planning out a project is to start by documenting it from the top level.
- This is the documentation give to someone who wanted to know how to use the tool but didn’t care about creating the tool.
- This approach is sometimes called “README Driven Development”. Whenever we write documentation in a Python project, we should be using `reStructuredText`.
- We use this specific markup format because there are tools in the Python ecosystem that can read this text and render documentation in a standardized way.


```c
// ~/code/pgbackup/README.rst

pgbackup
========

CLI for backing up remote PostgreSQL databases locally or to AWS S3.


Preparing for Development
-------------------------

1. Ensure ``pip`` and ``pipenv`` are installed
2. Clone repository: ``git clone git@github.com:example/pgbackup``
3. ``cd`` into repository
4. Fetch development dependencies ``make install``
5. Activate virtualenv: ``pipenv shell``


Usage
-----

Pass in a full database URL, the storage driver, and destination.


1. S3 Example w/ bucket name:

::

    $ pgbackup postgres://bob@example.com:5432/db_one --driver s3 backups


2. Local Example w/ local path:

::

    $ pgbackup postgres://bob@example.com:5432/db_one --driver local /var/local/db_one/backups


Running Tests
-------------

1. Run tests locally using make if virtualenv is active:

::

    $ make

2. If virtualenv isn’t active then use:

::

    $ pipenv run make
```

### 5. Our Initial Commit

```c
$ git add --all .
$ git status
On branch master
No commits yet
Changes to be committed:
  (use "git rm --cached <file>..." to unstage)
	new file:   .gitignore
	new file:   Pipfile
	new file:   README.rst

$ git commit -m 'Initial commit'
```

---

## Initial Project Layout

### 1. Create Package Layout
- few specific places to put code in this project:
- In `src/pgbackup` directory: project’s business logic.
    - use a special file called `__init__.py`
- In `tests` directory: put automated tests here.
    - use a generically named, hidden file. `.keep`

- We’re not going to write the code that goes in these directories just yet, but we are going to create them and put some empty files in so that we can make a git commit that contains these directories.


```c
(pgbackup-E7nj_BsO) $ mkdir -p src/pgbackup tests
(pgbackup-E7nj_BsO) $ touch src/pgbackup/__init__.py tests/.keep
```

### 2. write `setup.py`

- One of the requirements for an installable Python package is a `setup.py` file at the root of the project.
- utilize `setuptools` to specify
    - how our project is to be installed
    - define its metadata.


```py
# $ vim ~/code/pgbackup/setup.py

from setuptools import setup, find_packages

with open('README.rst', encoding='UTF-8') as f:
    readme = f.read()

setup(
    name='pgbackup',
    version='0.1.0',
    description='Database backups locally or to AWS S3.',
    long_description=readme,
    author='grace',
    author_email='Lgraceye@hotmail.com',
    packages=find_packages('src'),
    package_dir={'': 'src'},
    install_requires=[]
)

# For the most part, this file is metadata,
# but the packages, package_dir, and install_requires parameters of the setup function define where setuptools will look for our source code and what other packages need to be installed for our package to work.
```


- make sure didn’t mess up thing in setup.py, install package as a development package using pip.


```c
$ pipenv shell

(pgbackup-E7nj_BsO) $ pip install -e .
Obtaining file:///home/user/code/pgbackup
Installing collected packages: pgbackup
  Running setup.py develop for pgbackup
Successfully installed pgbackup
// everything worked, and we won’t need to change our setup.py for awhile.


// uninstall pgbackup since it doesn’t do anything yet:
(pgbackup-E7nj_BsO) $ pip uninstall pgbackup
```


### 3. Makefile
- in `README.rst` file, run tests is able to simply run make from terminal.
- need to have a `Makefile`.
- create a second make task that can be used to setup the `virtualenv` and install dependencies using `pipenv` also.


```c
// $ vim ~/code/pgbackup/Makefile
```

```c
.PHONY: default install test

default: test

install:
    pipenv install --dev --skip-lock

test:
    PYTHONPATH=./src pytest
```

### 4. make commit

```c
(pgbackup-E7nj_BsO) $ git add --all .
(pgbackup-E7nj_BsO) $ git commit -m 'Structure project with setup.py and Makefile'
```

---

## Introduction to TDD and First Tests

### 1. Installing pytest
pytest
- testing framework.
- install by `pipenv` and specify that this is a “dev” dependency:

```c
(pgbackup-E7nj_BsO) $ pipenv install --dev pytest
...
Adding pytest to Pipfiles [dev-packages]…
Locking [dev-packages] dependencies…
Locking [packages] dependencies…
Updated Pipfile.lock (5c8539)!

// Now the line that we wrote in our Makefile that utilized the pytest, CLI will work.

$ cat Pipfile

[[source]]
name = "pypi"
url = "https://pypi.org/simple"
verify_ssl = true

[dev-packages]
pytest = "*"

[packages]

[requires]
python_version = "3.6"
```

### 2. Write First Tests

write a few failing tests using pytes
- tests will be functions with names that start with `test_`
- As long as we name the functions properly, the test runner should find and run them.

A test that shows that the CLI fails if no driver is specified.
A test that shows that the CLI fails if there is no destination value given.
A test that shows, given a driver and a destination, that the CLI’s returned Namespace has the proper values set.

At this point, we don’t even have any source code files, but that doesn’t mean that we can’t write code that demonstrates how we would like our modules to work.

module: `cli`
- have a `create_parser` function: returns an `ArgumentParser` configured for desired use.
- write some tests that exercise `cli.create_parser` and ensure that our ArgumentParser works as expected.
- The name of test file is important; `test_cli.py`.


```py
# $ vim ~/code/pgbackup/tests/test_cli.py

import pytest
from pgbackup import cli

url = "postgres://bob:password@example.com:5432/db_one"

def test_parser_without_driver():
    """
    Without a specified driver the parser will exit
    """
    with pytest.raises(SystemExit):
        parser = cli.create_parser()
        parser.parse_args([url])

def test_parser_with_driver():
    """
    The parser will exit if it receives a driver
    without a destination
    """
    parser = cli.create_parser()
    with pytest.raises(SystemExit):
        parser.parse_args([url, "--driver", "local"])

def test_parser_with_driver_and_destination():
    """
    The parser will not exit if it receives a driver
    with a destination
    """
    parser = cli.create_parser()
    args = parser.parse_args([url, "--driver","local","/some/path"])
    assert args.driver == "local"
    assert args.destination == "/some/path"
```

### 3. Running Tests

make sure our virtualenv is active and run them:
`$ pipenv shell`

```c
(pgbackup-E7nj_BsO) $ make

PYTHONPATH=./src pytest
======================================= test session starts =======================================
platform linux -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0
rootdir: /home/user/code/pgbackup, inifile:
collected 0 items / 1 errors

============================================= ERRORS ==============================================
___ ERROR collecting tests/test_cli.py ____
ImportError while importing test module '/home/user/code/pgbackup/tests/test_cli.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_cli.py:3: in
    from pgbackup import cli
    E   ImportError: cannot import name 'cli'
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    ===================================== 1 error in 0.11 seconds =====================================
    make: *** [test] Error 2
```

---

## Implementing CLI Guided By Tests

`$ make`

`test_cli.py`:

```py
import pytest
from pgbackup import cli

url = "postgres://bob:password@example.com:5432/db_one"

def test_parser_without_driver():
    with pytest.raises(SystemExit):
        parser = cli.create_parser()
        parser.parse_args([url])

def test_parser_with_driver():
    parser = cli.create_parser()
    with pytest.raises(SystemExit):
        parser.parse_args([url, "--driver", "local"])

def test_parser_with_driver_and_destination():
    parser = cli.create_parser()
    args = parser.parse_args([url, "--driver","local","/some/path"])
    assert args.driver == "local"
    assert args.destination == "/some/path"
```

### 1st error: `ImportError`

`ImportError: cannot import name 'cli'`

create `cli.py`

```c
$ touch src/pgbackup/cli.py
```

### 2nd error: `AttributeError`

```py
AttributeError: module 'pgbackup.cli' has no attribute 'create_parser'
```

create `cli.py` content

```py
from argparse import ArgumentParser

def create_parser():
    parser = ArgumentParser()
    return parser
```

### 3rd error: `pytest: error: unrecognized arguments:`

`pytest: error: unrecognized arguments: postgres://bob:password@example.com:5432/db_one --driver local /some/path`

edit `cli.py` content: Creating First Class

```py
from argparse import ArgumentParser

class DriverAction(Action):
    def call(self, parser, namespace, values, option_string=None):
        driver, destination = values
        namespace.driver = driver.lower()
        namespace.destination = destination

def create_parser():
    parser = ArgumentParser(description="Back up PostgreSQL databases locally or to AWS S3. ")
    parser.add_argument("url", helo="URL of database to backup")
    parser.add_argument("--driver",
            help="how & where to store backup",
            nargs=2,
            action=DriverAction,
            required=True)
    return parser
```

### Adding More Tests

`test_cli.py`: add the following

```py
def test_parser_with_unknown_drivers():
    "The parser will exit if the driver name is unknown."
    parser = cli.create_parser()
    with pytest.raises(SystemExit):
        parser.parse_args([url, "--driver", "azure", "destination"])

def test_parser_with_known_drivers():
    "The parser will not exit if the driver name is known."
    parser = cli.create_parser()
    for driver in ['local', 's3']:
        assert parser.parse_args([url, "--driver", driver, "destination"])        
```

### Adding Driver Type Validation

edit `cli.py` content:
- import `Action`
- Creating First Class

```py
from argparse import Action, ArgumentParser

class DriverAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        driver, destination = values
        if driver.lower() not in known_drivers:
            parser.error("Unknown driver. Available drivers are 'local' & 's3'")
        namespace.driver = driver.lower()
        namespace.destination = destination

def create_parser():
    parser = ArgumentParser(description="Back up PostgreSQL databases locally or to AWS S3. ")
    parser.add_argument("url", help="URL of database to backup")
    parser.add_argument("--driver",
            help="how & where to store backup",
            nargs=2,
            action=DriverAction,
            required=True)
    return parser
```

### Removing Test Duplication Using pytest.fixture


`test_cli.py`:

```py
import pytest
from pgbackup import cli

@pytest.fixture
def parser():
    return cli.create_parser()

url = "postgres://bob:password@example.com:5432/db_one"

def test_parser_without_driver(parser):
    with pytest.raises(SystemExit):
        parser.parse_args([url])

def test_parser_with_driver(parser):
    with pytest.raises(SystemExit):
        parser.parse_args([url, "--driver", "local"])

def test_parser_with_unknown_drivers(parser):
    with pytest.raises(SystemExit):
        parser.parse_args([url, "--driver", "azure", "destination"])

def test_parser_with_known_drivers(parser):
    for driver in ['local', 's3']:
        assert parser.parse_args([url, "--driver", driver, "destination"])

def test_parser_with_driver_and_destination(parser):
    args = parser.parse_args([url, "--driver","local","/some/path"])
    assert args.driver == "local"
    assert args.destination == "/some/path"
```

### git

```c
git status
git add --all .
git commit -m 'Implement CLI module'
```

---

## Introduction to Mocking in Tests

### 1. Install pytest-mock
Before we can learn how to use mocking in our tests, we need to install the pytest-mock package. This will pull in a few packages for us, and mainly provide us with a mocker fixture that we can inject into our tests:

```c
$ pipenv install --dev pytest-mock
```

### 2. Writing Tests With Mocking
We’re going to put all of the Postgres related logic into its own module called `pgdump`, and we’re going to begin by writing our tests. We want this module to do the following:

1. Make a call out to `pg_dump` using `subprocess.Popen`.
2. Returns the `subprocess` that `STDOUT` can be read from.

We know how to use the `subprocess` module, but we haven’t used `subprocess.Popen` yet.

Behind the scenes, the functions that we already know use `Popen`, and wait for it to finish.

We’re going to use this instead of run, because we want to continue running code instead of waiting, right until we need to write the contents of `proc.stdout` to a file or S3.

- To ensure that our code runs the proper third-party utilities, we’re going to use `mocker.patch` on the `subprocess.Popen` constructor.
    - This will substitute in a different implementation that holds onto information like the number of times the function is called and with what arguments.

```py
# tests/test_pgdump.py
import pytest
import subprocess
from pgbackup import pgdump

url = "postgres://bob:password@example.com:5432/db_one"

def test_dump_calls_pg_dump(mocker):
    """
    Utilize pg_dump with the database URL
    """
    mocker.patch('subprocess.Popen')
    assert pgdump.dump(url)
    subprocess.Popen.assert_called_with(['pg_dump', url], stdout=subprocess.PIPE)
```

The arguments that we’re passing to `assert_called_with` will need to match what is being passed to `subprocess.Popen` when we exercise `pgdump.dump(url)`.

---

## Implementing PostgreSQL Interaction


### 1st error: `ImportError`

`ImportError: cannot import name 'pgdump'`

`$ vim /src/pgbackup/pgdump.py`


### 2nd error: `AttributeError`
`AttributeError: module 'pgbackup.pgdump' has no attribute 'dump'`

```py
# $ vim /src/pgbackup/pgdump.py

def dump():
    pass
```

### 3rd error `TypeError`

`TypeError: dump() takes 0 positional arguments but 1 was given`

```py
# $ vim /src/pgbackup/pgdump.py
def dump(url):
    pass 1
```

### 4th error

```py
    mocker.patch('subprocess.Popen')
    assert pgdump.dump(url)
>       subprocess.Popen.assert_called_with(['pg_dump', url], stdout=subprocess.PIPE)
E       AssertionError: Expected call: Popen(['pg_dump', 'postgres://bob:password@example.com:5432/db_one'], stdout=-1)
E       Not called

--------------------------

# $ vim /src/pgbackup/pgdump.py
import subprocess

def dump(url):
    return subprocess.Popen(['pg_dump', url], stdout=subprocess.PIPE)
```


```py
# tests/test_pgdump.py
import pytest
import subprocess
from pgbackup import pgdump

url = "postgres://bob:password@example.com:5432/db_one"

def test_dump_calls_pg_dump(mocker):
    "Utilize pg_dump with the database URL."
    mocker.patch('subprocess.Popen')
    assert pgdump.dump(url)
    subprocess.Popen.assert_called_with(['pg_dump', url], stdout=subprocess.PIPE)

def test_dump_handles_oserror(mocker):
    "pgdump.dump returns a reasonable error if pg_dump isn't installed."
    mocker.patch('subprocess.Popen', side_effect=OSError("no such file"))
    with pytest.raises(SystemExit):
        pgdump.dump(url)
```


### Implementing Error Handling
Since we know that subprocess.Popen can raise an OSError, we’re going to wrap that call in a try block, print the error message, and use sys.exit to set the error code:

```py
src/pgbackup/pgdump.py

import sys
import subprocess

def dump(url):
    try:
        return subprocess.Popen(['pg_dump', url], stdout=subprocess.PIPE)
    except OSError as err:
        print(f"Error: {err}")
        sys.exit(1)
```

### Manual Testing

We can have a certain amount of confidence in our code because we’ve written tests that cover our expected cases, but since we used patching, we don’t know that it works. Let’s manually load our code into the python REPL to test it out:
```py
(pgbackup-E7nj_BsO) $ PYTHONPATH=./src python
>>> from pgbackup import pgdump
>>> dump = pgdump.dump('postgres://demo:password@54.245.63.9:80/sample')
# dump = subprocess.Popen(['pg_dump', url], stdout=subprocess.PIPE)

>>> f = open('dump.sql', 'w+b')
>>> f.write(dump.stdout.read())
>>> f.close()
Note: We needed to open our dump.sql file using the w+b flag because we know that the .stdout value from a subprocess will be a bytes object and not a str.
```
If we exit and take a look at the contents of the file using cat, we should see the SQL output. With the pgdump module implemented, it’s now a great time to commit our code.

---

## Implementing Local File Storage

### Writing Local File Tests

what local storage driver needs to do
- Take in one “readable” object and one, local, “writeable” object.
- Write the contents of the “readable” object to the “writeable” object.
- don’t need our inputs to be file objects.
    - They need to implement some of the same methods that a file does, like read and write, but they don’t have to be file objects.
- For testing purposes, use the `tempfile` package to create a `TemporaryFile` to act as “readable” and another `NamedTemporaryFile` to act as “writeable”.
- pass them both into function, and assert after the fact that the contents of the “writeable” object match what was in the “readable” object:

```py
# tests/test_storage.py

import tempfile
from pgbackup import storage

def test_storing_file_locally():
    "Writes content from one file-like to another"

    infile = tempfile.TemporaryFile('r+b')
    infile.write(b"Testing")
    infile.seek(0)

    outfile = tempfile.NamedTemporaryFile(delete=False)

    storage.local(infile, outfile)
    with open(outfile.name, 'rb') as f:
        assert f.read() == b"Testing"
```

### Implement Local Storage

- call close on the “writeable” file to ensure that all of the content gets written (the database backup could be quite large):

```py
#src/pgbackup/storage.py

def local(infile, outfile):
    outfile.write(infile.read())
    outfile.close()
    infile.close()
```

---

## Implementing AWS Interaction

### Installing boto3
To interface with AWS (S3 specifically), use the boto3 package.

```c
// install this to virtualenv using pipenv:
(pgbackup-E7nj_BsO) $ pipenv install boto3
```

### Configuring AWS Client
The boto3 package works off of the same configuration file that you can use with the official aws CLI.
- leave `virtualenv` and install the `awscli` package for user.
- use its configure command to set up config file:

```c
(pgbackup-E7nj_BsO) $ exit

$ mkdir ~/.aws

$ pip3.6 install --user awscli

$ aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]:
Default output format [None]:

// The exec $SHELL portion reload the shell to ensure that the configuration changes are picked up.
$ exec $SHELL

// make sure to reactivate development virtualenv again:
$ pipenv shell
```


### Implementing S3 Strategy

```py
# src/pgbackup/storage.py

def local(infile, outfile):
    outfile.write(infile.read())
    outfile.close()
    infile.close()

def s3(client, infile, bucket, name):
    client.upload_fileobj(infile, bucket, name)
```


### Writing S3 test
write tests for our S3 interaction.
- To limit the explicit dependencies that we have, we’re going to have the following parameters to our storage.s3 function:
- A client object that has an `upload_fileobj` method.
- A boto3 client meets this requirement, but in testing, we can pass in a “mock” object that implements this method.
- A file-like object (responds to read).
- An S3 bucket name as a string.
- The name of the file to create in S3.

```py
# need an infile for all tests, extract a fixture for that also.

# tests/test_storage.py

import tempfile
import pytest

from pgbackup import storage

@pytest.fixture
def infile():
    infile = tempfile.TemporaryFile('r+b')
    infile.write(b"Testing")
    infile.seek(0)
    return infile

def test_storing_file_locally(infile):
    "Writes content from one file-like to another"
    outfile = tempfile.NamedTemporaryFile(delete=False)
    storage.local(infile, outfile)
    with open(outfile.name, 'rb') as f:
        assert f.read() == b"Testing"

def test_storing_file_on_s3(mocker, infile):
    "Writes content from one readable to S3"
    client = mocker.Mock()
    storage.s3(client, infile, "bucket", "file-name")
    client.upload_fileobj.assert_called_with( infile, "bucket", "file-name")
```


### Manually Testing S3 Integration

manually test uploading a file to S3 using `storage.s3` function.
- create an `example.txt` file
- load into a Python REPL with our code loaded:

```c
(pgbackup-E7nj_BsO) $ echo "UPLOADED" > example.txt

(pgbackup-E7nj_BsO) $ PYTHONPATH=./src python
>>> import boto3
>>> from pgbackup import storage
>>> client = boto3.client('s3')
>>> infile = open('example.txt', 'rb')
>>> storage.s3(client, infile, 'pyscripting-db-backups', infile.name)

// check our S3 console, we should see the file there.
// remove the example.txt file and then commit these changes:

(pgbackup-E7nj_BsO) $ rm example.txt
(pgbackup-E7nj_BsO) $ git add .
(pgbackup-E7nj_BsO) $ git commit -m 'Implement S3 interactions'
```

---

## Wiring the Units Together

### Add “console_script” to project
- make project create a console script for us when user runs `pip install`.
- similar as made executables before, except don’t need to manually do the work.
- To do this, need to add an `entry point` in our setup.py:

```py
# setup.py
from setuptools import setup, find_packages

with open('README.rst', encoding='UTF-8') as f:
    readme = f.read()

setup(
    name='pgbackup',
    version='0.1.0',
    description='Database backups locally or to AWS S3.',
    long_description=readme,
    author='grace',
    author_email='Lgraceye@hotmail.com',
    packages=find_packages('src'),
    package_dir={'': 'src'},
    install_requires=['boto3'],
    entry_points={
        'console_scripts': [
            'pgbackup=pgbackup.cli:main',
            ],
        }
    )
```

referencing `cli` module with a `:` and a `main`.
- `main` is the function that we need to create now.


### Wiring The Units Together
`main` function is going to go in the `cli` module, and it needs to do the following:
- Import the `boto3` package.
- Import our `pgdump` and `storage` modules.
- Create a `parser` and parse the arguments.
- Fetch the database dump.
- Depend on the driver type do one of the follow:
    - create a `boto3 S3 client` and use `storage.s3`
    - open a `local file` and use `storage.local`


```py
# src/pgbackup/cli.py
from argparse import Action, ArgumentParser

class DriverAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        driver, destination = values
        if driver.lower() not in known_drivers:
            parser.error("Unknown driver. Available drivers are 'local' & 's3'")
        namespace.driver = driver.lower()
        namespace.destination = destination

def create_parser():
    parser = ArgumentParser(description="Back up PostgreSQL databases locally or to AWS S3. ")
    parser.add_argument("url", help="URL of database to backup")
    parser.add_argument("--driver",
            help="how & where to store backup",
            nargs=2,
            action=DriverAction,
            required=True)
    return parser


def main():
    import boto3
    from pgbackup import pgdump, storage

    args = create_parser().parse_args()
    dump = pgdump.dump(args.url)
    if args.driver == 's3':
        client = boto3.client('s3')
        # TODO: create a better name based on the database name and the date
        storage.s3(client, dump.stdout, args.destination, 'example.sql')
    else:
        outfile = open(args.destination, 'wb')
        storage.local(dump.stdout, outfile)
```

test it out:

```c
$ pipenv shell
(pgbackup-E7nj_BsO) $ pip install -e .
(pgbackup-E7nj_BsO) $ pgbackup --driver local ./local-dump.sql postgres://demo:password@54.245.63.9:80/sample
(pgbackup-E7nj_BsO) $ pgbackup --driver s3 pyscripting-db-backups postgres://demo:password@54.245.63.9:80/sample
```

### Reviewing the Experience

fix:
- Generate a good file name for S3
- Create some output while the writing is happening
- Create a shorthand switch for --driver (-d)

### Generating a Dump File Name

For generating our filename, let’s put all database URL interactions in the pgdump module with a function name of dump_file_name. This is a pure function that takes an input and produces an output, so it’s a prime function for us to unit test.

```py
# tests/test_pgdump.py (partial)

def test_dump_file_name_without_timestamp():
    "pgdump.db_file_name returns the name of the database"
    assert pgdump.dump_file_name(url) == "db_one.sql"

def test_dump_file_name_with_timestamp():
    "pgdump.dump_file_name returns the name of the database"
    timestamp = "2017-12-03T13:14:10"
    assert pgdump.dump_file_name(url, timestamp) == "db_one-2017-12-03T13:14:10.sql"
```

We want the file name returned to be based on the database name, and it should also accept an optional timestamp. Let’s work on the implementation now:

```py
# src/pgbackup/pgdump.py

import sys
import subprocess

def dump(url):
    try:
        return subprocess.Popen(['pg_dump', url], stdout=subprocess.PIPE)
    except OSError as err:
        print(f"Error: {err}")
        sys.exit(1)

def dump_file_name(url, timestamp=None):
    db_name = url.split("/")[-1]
    db_name = db_name.split("?")[0]
    if timestamp:
        return f"{db_name}-{timestamp}.sql"
    else:
        return f"{db_name}.sql"
```

### Improving the CLI and Main Function

We want to add a shorthand -d flag to the driver argument, let’s add that to the create_parser function:

```py
# src/pgbackup/cli.py

def create_parser():
    parser = argparse.ArgumentParser(description="Back up PostgreSQL databases locally or to AWS S3.")
    parser.add_argument("url", help="URL of database to backup")
    parser.add_argument("--driver", "-d",
            help="how & where to store backup",
            nargs=2,
            metavar=("DRIVER", "DESTINATION"),
            action=DriverAction,
            required=True)
    return parser

# print a timestamp with time.strftime, generate a database file name, and print what we’re doing as we upload/write files.

def main():
    import time
    import boto3
    from pgbackup import pgdump, storage

    args = create_parser().parse_args()
    dump = pgdump.dump(args.url)

    if args.driver == 's3':
        client = boto3.client('s3')
        timestamp = time.strftime("%Y-%m-%dT%H:%M", time.localtime())
        file_name = pgdump.dump_file_name(args.url, timestamp)
        print(f"Backing database up to {args.destination} in S3 as {file_name}")
        storage.s3(client, dump.stdout, args.destination, file_name)
    else:
        outfile = open(args.destination, 'wb')
        print(f"Backing database up locally to {outfile.name}")
        storage.local(dump.stdout, outfile)</code></pre>
```

---

## Build and Share a Wheel Distribution

### Adding a setup.cfg

1. configure setuptools to not build the wheel for Python 2.
    - can’t build for Python2 because we used string interpolation.
    - put this configuration in a `setup.cfg`:

```py
# setup.cfg
[bdist_wheel]
python-tag = py36
```

2. to build wheel:

```c
(pgbackup-E7nj_BsO) $ python setup.py bdist_wheel
```

3. uninstall and re-install our package using the wheel file:

```c
(pgbackup-E7nj_BsO) $ pip uninstall pgbackup
(pgbackup-E7nj_BsO) $ pip install dist/pgbackup-0.1.0-py36-none-any.whl
```


### Install a Wheel From Remote Source (S3)
- use `pip` to install `wheels` from a local path
- can also install from a remote source over HTTP.
    - upload wheel to S3
    - then install the tool outside of virtualenv from S3:

```py
(pgbackup-E7nj_BsO) $ python
>>> import boto3
>>> f = open('dist/pgbackup-0.1.0-py36-none-any.whl', 'rb')
>>> client = boto3.client('s3')
>>> client.upload_fileobj(f, 'pyscripting-db-backups', 'pgbackup-0.1.0-py36-none-any.whl')
>>> exit()
```

go into the S3 console and make this file public to download it to install.

exit our `virtualenv` and install `pgbackup` as a user package:

```c
(pgbackup-E7nj_BsO) $ exit
$ pip3.6 install --user https://s3.amazonaws.com/pyscripting-db-backups/pgbackup-0.1.0-py36-none-any.whl
$ pgbackup --help
```





















.
v
