
# 24.13. Unicode for non-English characters

[toc]


## 24.13. Unicode for characters

Python’s strings are in unicode, which allows for characters to be from a much larger alphabet, including more than 75,000 ideographic characters used in Chinese, Japanese, and Korean alphabets.

Everything works fine inside Python, for operations like slicing and appending and concatenating strings and using `.find()` or the `in` operator.

output in terminal, (output) window will typically be set up to display characters only from a restricted set of languages (perhaps just English). If you issue a print statement on a string containing other characters, it may not display correctly in your terminal window. Indeed, you may get an error message. We will offer a workaround later on this page.

to store unicode text in a file, you have to choose an “encoding”. This is analogous to the encoding of special characters in a URL string, but not the same. In a file, each unicode character has to be encoded as one or more “bytes” for storage in a file. We have avoided low-level details about data encodings until now, but understanding a little about bits and bytes will help make sense of this.

A `bit` is a `BInary digiT`
- a single value restricted to two (binary) possibilities, 0 or 1.
- Computers store bits as electrical charges (high or low voltage) or as magnetic polarities, or some other way that we need not be concerned about.
- A sequence of eight 0-1 bits is called a `byte`. For example: 01001000.

There are 2^^8=256 distinct eight-bit bytes. If only 256 possible letters in alphabet, could simply encode each letter as one of the available bytes.

When there are 75,000 possible characters, they can’t all be encoded with a single byte, because there are only 256 distinct bytes (eight-bit sequences). There are many possible encodings. The one you will be most likely to encounter, using REST APIs, is called UTF-8. A single unicode character is mapped to a sequence of up to four bytes.


read a UTF-8 encoded text

get the contents using `.read()` or `.readlines()`
- need to “decode” the contents to turn it into a proper unicode string to read and use.

the `requests` module normally handle this automatically.
- When fetch a webpage that is in json format, the webpage will have a `header` called <font color=red> ‘content-type’ </font> that will say something like `application/json; charset=utf8`.
- If it specifies the utf8 character set in that way, the `requests` module will automatically decode the contents into unicode:
- `requests.get('url').text` will yield a string, with each of those sequences of one to four bytes coverted into a single character.

If get json-formatted text that is utf-encoded but the requests module hasn’t decoded it,
- the `json.loads()` function call can decode. `loads()` takes an optional parameter, encoding.
- Its default value is ‘utf-8’, don’t need to specify it unless the text received was in other encoding than ‘utf-8’.


print or write the contents to a file.

If print, and terminal window is not set up to display that language, you may get a strange output, or an error message.

If to write to a file with unicode strings, may get an error.
- When you write a unicode string to a file, Python tries to encode it in ASCII. If there is a non-ASCII character, the execution fails and raises an error that looks like this: `UnicodeEncodeError: 'ascii' codec can't encode character u'\xea' in position 1: ordinal not in range(128).`

solution

- use the Python method to encode the string
  - `s.encode('utf-8')`, encode string s as utf-8.
  - encode non-ASCII characters with multiple character sequences that are difficult for people to read but can decoded back into single Unicode characters. This is often the best way.

- if just have a few stray characters, replace any non-ASCII characters with question marks.
  - `s.encode('ascii', 'replace')``
  - replacing characters with question marks destroys some of the information, but it may be helpful in some circumstances.



























.
